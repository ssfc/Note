# Self-attention slides of Lee Hongyi

https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf

Input is a vector 很好理解，因为刚刚做完word-embedding. (2023年7月3日)

Input is a set of vectors 因为输入的一句话包含多个word, 每个word是一个vector, 合起来自然是a set of vectors. => 那input就是Matrix了。(2023年7月3日)

Can be either input or a hidden layer. 如果是左下和右下的attention, a自然是input layer; 如果是右上的attention, a明显是hidden layer. (2023年7月3日)

### Q: 在线性代数中，一个一维向量一般横着写还是竖着写？

一维向量是指只有一个分量的向量，它可以用一个实数来表示。一维向量可以横着写也可以竖着写，但是通常为了和矩阵相区分，我们会把一维向量写成列向量，即竖着写。例如，如果x是一个一维向量，那么我们可以写成

x = [x1]

其中x1是一个实数。这样做的好处是可以方便地进行标量乘法和矩阵乘法等运算。例如，如果a是一个标量，那么ax就是把x1乘以a；如果A是一个m×1的矩阵，那么Ax就是把A的每个元素乘以x1。

### Q: 是否可以这样理解，数学中的一维向量一般写成列向量，计算机中（比如python或matlab）的一维向量一般写成行向量？

是的，你的理解基本上是正确的。

在数学中，一维向量通常以列向量的形式表示，以强调向量的纵向性质和线性代数中的操作。在线性代数中，矩阵乘法等运算符号的定义也依赖于向量的纵向排列。

而在一些计算机编程环境中，如Python和MATLAB，一维向量通常以行向量的形式表示。这是因为在计算机科学和编程中，行向量更符合通常的数据表示和存储方式，以及一些编程语言中的数组和矩阵操作的约定。在这些编程环境中，行向量更方便处理和操作。

需要注意的是，在某些情况下，数学和计算机编程中的向量表示方式可能会有所不同，因此在进行数学计算和编程时，需要根据具体环境和约定来选择合适的向量表示方式。在数学和线性代数领域，以及计算机编程和数据处理领域，了解和适应这种表示方式的差异是很重要的。

### Q: 是否可以这样理解，在word-embedding后，input matrix的每一行代表一个word?

在词嵌入（word embedding）之后，input matrix的每一行通常代表一个词（word）或一个单词的嵌入表示。

词嵌入是将词语映射到一个高维向量空间的过程，其中每个词语被表示为一个实数向量。在自然语言处理中，词嵌入被广泛应用于词义表示、文本分类、机器翻译等任务。

在进行词嵌入之后，通常会得到一个嵌入矩阵（embedding matrix）。该矩阵的每一行对应一个词语的嵌入表示。每个词语的嵌入向量包含在矩阵的相应行中。

举例来说，假设我们有一个包含10000个不同词语的词汇表，并使用100维的词嵌入表示每个词语。那么对应的嵌入矩阵将是一个10000行、100列的矩阵。矩阵的每一行表示词汇表中对应词语的嵌入向量。

因此，可以说在词嵌入之后，input matrix的每一行代表一个词（word）或一个单词的嵌入表示。这样的表示方式有助于在后续的自然语言处理任务中对词语进行处理和分析。

### Q: 什么是多头注意力(multi-head attention)?

多头注意力（Multi-head attention）是 Transformer 模型中的一种机制，用于增强模型在处理序列数据时的表征能力和学习能力。

在 Transformer 模型中，注意力机制被广泛应用于捕捉输入序列中不同位置之间的依赖关系和重要性。多头注意力机制通过并行地应用多个注意力头来处理输入序列，从而允许模型在不同的表示子空间中进行自注意力计算。

具体而言，多头注意力将输入序列进行线性变换，然后将变换后的结果分成多个头（head）。每个头都具有自己的权重矩阵，通过计算注意力分数来对输入序列中的位置进行加权求和。最后，多个头的输出被连接起来并经过另一个线性变换，得到最终的多头注意力输出。

通过引入多个头，多头注意力能够学习到不同的注意力表示，从而增加模型对不同关注点和语义信息的建模能力。每个头可以关注序列中的不同部分，并在整个模型中进行信息传递和组合。这样的设计有助于模型更好地捕捉序列中的局部和全局关系，并提升模型在各种自然语言处理任务中的性能。

多头注意力是 Transformer 模型中的一个重要组件，它在编码器和解码器中被广泛使用，用于处理输入序列和生成输出序列。通过使用多头注意力，Transformer 模型能够有效地处理长距离依赖关系和复杂的序列建模任务。

### Q: 是否可以这样理解，transformer中，被乘数是向量，乘数是矩阵？

在 Transformer 模型中，乘法运算的操作是矩阵乘法（Matrix Multiplication）。确切地说，在自注意力机制（Self-Attention）的步骤中，乘数是一个矩阵，被乘数是一个向量。

在自注意力机制中，我们有三个输入矩阵，即查询（Query）、键（Key）和值（Value）矩阵。这些矩阵用于计算注意力权重，并与值矩阵相乘以得到最终的输出。

- 查询矩阵（Q）：它是一个形状为（序列长度，特征维度）的矩阵，代表了当前位置的向量表示。
- 键矩阵（K）：它也是一个形状为（序列长度，特征维度）的矩阵，代表了所有位置的向量表示。
- 值矩阵（V）：它是一个形状为（序列长度，特征维度）的矩阵，代表了所有位置的向量表示。

在自注意力机制中，通过计算查询矩阵（Q）和键矩阵（K）之间的点积，然后进行归一化操作，得到注意力权重。这个权重矩阵与值矩阵（V）相乘，得到最终的输出结果。

总结起来，被乘数是一个向量（查询矩阵Q的某一行），乘数是一个矩阵（键矩阵K和值矩阵V）。通过乘法运算，我们得到了经过注意力加权的值矩阵，用于最终的输出。

需要注意的是，在 Transformer 中还存在其他的线性变换和矩阵乘法操作，用于进行位置编码、前馈神经网络等步骤。因此，整个 Transformer 模型涉及到多个矩阵乘法操作，用于转换和组合输入数据。

### Q: 被乘数可以理解成输入序列中的一个单词吗？

在 Transformer 模型中，被乘数可以理解为输入序列中的一个位置或一个单词的表示。

在自注意力机制的步骤中，输入序列被表示为一组向量，每个向量对应输入序列中的一个位置（可以是一个单词或一个字符）。在计算自注意力时，我们需要为每个位置生成查询（Query）向量、键（Key）向量和值（Value）向量。

- 被乘数：在自注意力机制中，被乘数通常是查询（Query）向量，表示当前位置的向量表示。这个向量用于与其他位置的键（Key）向量计算相似性，并获得注意力权重。

- 乘数：乘数包括键（Key）矩阵和值（Value）矩阵。键矩阵包含了所有位置的向量表示，而值矩阵也包含了所有位置的向量表示。这两个矩阵将与查询向量进行计算，以生成最终的注意力权重。

通过将查询向量与键矩阵进行点积运算，并进行归一化操作，我们可以得到注意力权重。然后，将注意力权重与值矩阵相乘，得到经过注意力加权的值。这个注意力加权的值将用作自注意力机制的输出。

因此，可以将被乘数视为输入序列中的一个单词或位置的表示，用于计算自注意力权重和生成注意力加权的值。这个过程在 Transformer 模型中的每个位置都会进行，以捕捉输入序列中不同位置的关联性和重要性。