# Self-attention slides of Lee Hongyi

https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf

Input is a vector 很好理解，因为刚刚做完word-embedding. (2023年7月3日)

Input is a set of vectors 因为输入的一句话包含多个word, 每个word是一个vector, 合起来自然是a set of vectors. => 那input就是Matrix了。(2023年7月3日)

Can be either input or a hidden layer. 如果是左下和右下的attention, a自然是input layer; 如果是右上的attention, a明显是hidden layer. (2023年7月3日)

从纸上算一下page 15. 

### Q: 在线性代数中，一个一维向量一般横着写还是竖着写？

一维向量是指只有一个分量的向量，它可以用一个实数来表示。一维向量可以横着写也可以竖着写，但是通常为了和矩阵相区分，我们会把一维向量写成列向量，即竖着写。例如，如果x是一个一维向量，那么我们可以写成

x = [x1]

其中x1是一个实数。这样做的好处是可以方便地进行标量乘法和矩阵乘法等运算。例如，如果a是一个标量，那么ax就是把x1乘以a；如果A是一个m×1的矩阵，那么Ax就是把A的每个元素乘以x1。

### Q: 是否可以这样理解，数学中的一维向量一般写成列向量，计算机中（比如python或matlab）的一维向量一般写成行向量？

是的，你的理解基本上是正确的。

在数学中，一维向量通常以列向量的形式表示，以强调向量的纵向性质和线性代数中的操作。在线性代数中，矩阵乘法等运算符号的定义也依赖于向量的纵向排列。

而在一些计算机编程环境中，如Python和MATLAB，一维向量通常以行向量的形式表示。这是因为在计算机科学和编程中，行向量更符合通常的数据表示和存储方式，以及一些编程语言中的数组和矩阵操作的约定。在这些编程环境中，行向量更方便处理和操作。

需要注意的是，在某些情况下，数学和计算机编程中的向量表示方式可能会有所不同，因此在进行数学计算和编程时，需要根据具体环境和约定来选择合适的向量表示方式。在数学和线性代数领域，以及计算机编程和数据处理领域，了解和适应这种表示方式的差异是很重要的。

### Q: 是否可以这样理解，在word-embedding后，input matrix的每一行代表一个word?

在词嵌入（word embedding）之后，input matrix的每一行通常代表一个词（word）或一个单词的嵌入表示。

词嵌入是将词语映射到一个高维向量空间的过程，其中每个词语被表示为一个实数向量。在自然语言处理中，词嵌入被广泛应用于词义表示、文本分类、机器翻译等任务。

在进行词嵌入之后，通常会得到一个嵌入矩阵（embedding matrix）。该矩阵的每一行对应一个词语的嵌入表示。每个词语的嵌入向量包含在矩阵的相应行中。

举例来说，假设我们有一个包含10000个不同词语的词汇表，并使用100维的词嵌入表示每个词语。那么对应的嵌入矩阵将是一个10000行、100列的矩阵。矩阵的每一行表示词汇表中对应词语的嵌入向量。

因此，可以说在词嵌入之后，input matrix的每一行代表一个词（word）或一个单词的嵌入表示。这样的表示方式有助于在后续的自然语言处理任务中对词语进行处理和分析。

### Q: 什么是多头注意力(multi-head attention)?

多头注意力（Multi-head attention）是 Transformer 模型中的一种机制，用于增强模型在处理序列数据时的表征能力和学习能力。

在 Transformer 模型中，注意力机制被广泛应用于捕捉输入序列中不同位置之间的依赖关系和重要性。多头注意力机制通过并行地应用多个注意力头来处理输入序列，从而允许模型在不同的表示子空间中进行自注意力计算。

具体而言，多头注意力将输入序列进行线性变换，然后将变换后的结果分成多个头（head）。每个头都具有自己的权重矩阵，通过计算注意力分数来对输入序列中的位置进行加权求和。最后，多个头的输出被连接起来并经过另一个线性变换，得到最终的多头注意力输出。

通过引入多个头，多头注意力能够学习到不同的注意力表示，从而增加模型对不同关注点和语义信息的建模能力。每个头可以关注序列中的不同部分，并在整个模型中进行信息传递和组合。这样的设计有助于模型更好地捕捉序列中的局部和全局关系，并提升模型在各种自然语言处理任务中的性能。

多头注意力是 Transformer 模型中的一个重要组件，它在编码器和解码器中被广泛使用，用于处理输入序列和生成输出序列。通过使用多头注意力，Transformer 模型能够有效地处理长距离依赖关系和复杂的序列建模任务。

### Q: 是否可以这样理解，transformer中，被乘数是向量，乘数是矩阵？

在 Transformer 模型中，乘法运算的操作是矩阵乘法（Matrix Multiplication）。确切地说，在自注意力机制（Self-Attention）的步骤中，乘数是一个矩阵，被乘数是一个向量。

在自注意力机制中，我们有三个输入矩阵，即查询（Query）、键（Key）和值（Value）矩阵。这些矩阵用于计算注意力权重，并与值矩阵相乘以得到最终的输出。

- 查询矩阵（Q）：它是一个形状为（序列长度，特征维度）的矩阵，代表了当前位置的向量表示。
- 键矩阵（K）：它也是一个形状为（序列长度，特征维度）的矩阵，代表了所有位置的向量表示。
- 值矩阵（V）：它是一个形状为（序列长度，特征维度）的矩阵，代表了所有位置的向量表示。

在自注意力机制中，通过计算查询矩阵（Q）和键矩阵（K）之间的点积，然后进行归一化操作，得到注意力权重。这个权重矩阵与值矩阵（V）相乘，得到最终的输出结果。

总结起来，被乘数是一个向量（查询矩阵Q的某一行），乘数是一个矩阵（键矩阵K和值矩阵V）。通过乘法运算，我们得到了经过注意力加权的值矩阵，用于最终的输出。

需要注意的是，在 Transformer 中还存在其他的线性变换和矩阵乘法操作，用于进行位置编码、前馈神经网络等步骤。因此，整个 Transformer 模型涉及到多个矩阵乘法操作，用于转换和组合输入数据。

### Q: 被乘数可以理解成输入序列中的一个单词吗？

在 Transformer 模型中，被乘数可以理解为输入序列中的一个位置或一个单词的表示。

在自注意力机制的步骤中，输入序列被表示为一组向量，每个向量对应输入序列中的一个位置（可以是一个单词或一个字符）。在计算自注意力时，我们需要为每个位置生成查询（Query）向量、键（Key）向量和值（Value）向量。

- 被乘数：在自注意力机制中，被乘数通常是查询（Query）向量，表示当前位置的向量表示。这个向量用于与其他位置的键（Key）向量计算相似性，并获得注意力权重。

- 乘数：乘数包括键（Key）矩阵和值（Value）矩阵。键矩阵包含了所有位置的向量表示，而值矩阵也包含了所有位置的向量表示。这两个矩阵将与查询向量进行计算，以生成最终的注意力权重。

通过将查询向量与键矩阵进行点积运算，并进行归一化操作，我们可以得到注意力权重。然后，将注意力权重与值矩阵相乘，得到经过注意力加权的值。这个注意力加权的值将用作自注意力机制的输出。

因此，可以将被乘数视为输入序列中的一个单词或位置的表示，用于计算自注意力权重和生成注意力加权的值。这个过程在 Transformer 模型中的每个位置都会进行，以捕捉输入序列中不同位置的关联性和重要性。

### Q: 在transformer中，权重矩阵WqWk是所有输入共享的吗？

在 Transformer 模型中，权重矩阵 Wq 和 Wk 是用于计算查询（Q）和键（K）的线性变换矩阵。每个注意力头（attention head）都有自己独立的权重矩阵 Wq 和 Wk。

在多头自注意力机制中，输入会通过多个注意力头进行并行处理，每个注意力头都有自己的权重矩阵 Wq 和 Wk。这种设计允许模型从不同的线性变换中学习不同的表示，以更好地捕捉输入之间的关系和语义信息。

因此，每个注意力头都有独立的权重矩阵 Wq 和 Wk，它们不是共享的，而是由模型进行学习和调整。这样可以增加模型的灵活性和表达能力。

### Q: 为什么需要多个注意力头？

使用多个注意力头的主要目的是增强模型的表达能力和学习能力。以下是使用多个注意力头的几个好处：

1. **多头并行处理**: 每个注意力头都可以并行地学习不同的表示，从而可以同时关注输入序列的不同部分。这有助于模型更好地捕捉输入之间的关系和语义信息。

2. **多头自注意力**: 在自注意力机制中，输入序列中的每个位置都可以与其他位置进行交互，获取全局的上下文信息。通过使用多个注意力头，模型可以从不同的表示子空间中学习并整合多个不同的上下文信息。

3. **丰富的表达能力**: 多头注意力允许模型以多种方式组合和加权不同的表示子空间，从而提供了更丰富的表达能力。每个注意力头可以专注于不同的语义特征，提取不同的关系和模式。

4. **稳定性和鲁棒性**: 多头注意力可以提高模型的稳定性和鲁棒性。通过多个独立的注意力头，模型可以从不同的角度考虑输入，并对输入中的噪声和不确定性具有一定的容忍度。

综上所述，多头注意力机制允许模型以并行的方式学习和整合多个表示，从而提高模型的表达能力、学习能力和鲁棒性。这在处理自然语言处理任务中，如机器翻译、文本分类和语义理解等方面特别有益。

### Q: 那么transformer注意力头的数量是多少？

Transformer 模型中的注意力头数量是一个可调整的超参数，可以根据具体任务和模型复杂度进行选择。一般来说，常用的注意力头数量是 8 或 16。

注意力头的数量决定了模型并行处理的能力和学习的复杂度。较少的注意力头数量可能导致模型的表达能力不足，而较多的注意力头数量可能增加模型的计算复杂度。

较少的注意力头数量可能导致模型的表达能力不足，而较多的注意力头数量可能增加模型的计算复杂度。

在实际应用中，选择合适的注意力头数量需要考虑以下因素：
- 数据集的规模和复杂度：对于大规模和复杂的数据集，较多的注意力头数量可能有助于模型更好地捕捉输入之间的关系和语义信息。
- 计算资源和效率要求：较多的注意力头数量可能需要更多的计算资源和时间，因此需要根据可用的资源和对模型的效率要求进行权衡。
- 模型的性能和实验结果：通过在验证集或测试集上进行实验，可以评估不同注意力头数量下模型的性能和效果，从而选择最佳的数量。

需要注意的是，注意力头数量不是唯一影响模型性能的因素，其他模型架构、超参数和训练策略也会对结果产生影响。因此，在选择注意力头数量时，需要综合考虑以上因素，并进行实验和评估来找到最佳的设置。

### Q: 也就是说，如果有8个注意力头，就有8套WqWk矩阵？

是的，每个注意力头都有自己独立的权重矩阵 Wq 和 Wk。如果你使用了 8 个注意力头，那么就会有 8 套独立的 Wq 和 Wk 矩阵。

在 Transformer 模型中，多头注意力机制通过将输入通过不同的线性变换矩阵（Wq、Wk、Wv）分别映射到不同的表示空间中，然后在每个注意力头上执行注意力计算。每个注意力头都有自己的权重矩阵，通过独立地学习这些矩阵，模型可以从不同的角度学习和整合输入的信息。

因此，如果有 8 个注意力头，就会有 8 套独立的权重矩阵 Wq 和 Wk，每个注意力头都会使用其中一套矩阵进行线性变换和注意力计算。这样，模型可以并行地从不同的表示子空间中学习和整合不同的语义特征，提高模型的表达能力和学习能力。

### Q: transformer的训练参数除了Wq、Wk、Wv，还有别的吗？

除了 Wq、Wk 和 Wv 这些权重矩阵之外，Transformer 模型的训练参数还包括以下重要的参数：

1. **嵌入层参数**: Transformer 模型通常包含输入嵌入层，用于将输入序列中的每个词或符号映射到连续的向量表示。嵌入层参数包括嵌入维度（embedding dimension）和词汇表大小（vocabulary size）等。

2. **位置编码参数**: 为了捕捉序列中的顺序信息，Transformer 模型引入了位置编码（Positional Encoding）层。位置编码参数用于将输入序列中每个位置的位置信息编码为向量表示，使模型能够区分不同位置的词汇。位置编码参数通常由模型学习或使用预定义的位置编码方法。

3. **前馈神经网络参数**: Transformer 模型中的每个注意力子层后面都有一个前馈神经网络（Feed-Forward Network）。前馈神经网络参数包括隐藏层大小、激活函数选择等。这些参数用于对注意力子层的输出进行非线性变换和映射。

4. **层数和注意力头数量**: Transformer 模型中的层数和注意力头数量也是训练参数。层数决定了模型的深度和复杂度，而注意力头数量决定了模型的并行处理能力和学习能力。这些参数通常在模型设计阶段进行选择，并根据具体任务和数据进行调优。

5. **学习率和优化算法参数**: 训练过程中的学习率和优化算法参数也是重要的训练参数。学习率决定了模型参数在每个训练步骤中的更新速度，而优化算法参数用于调整参数更新的方向和大小，常见的优化算法包括 Adam、SGD 等。

除了上述参数，还有一些其他的超参数和训练配置参数，如批量大小、正则化系数、丢弃率等，这些参数也会对模型的训练和性能产生影响。在实际应用中，选择和调整这些参数需要进行实验和调优，以获得最佳的模型性能和效果。

### Q: 在自注意力相关系数的计算中，需要计算每个单词所代表向量和自身的乘积吗？

在自注意力机制中，每个单词的向量并不直接与自身进行乘积。相反，自注意力机制通过计算每个单词与其他单词之间的相似度得到一个相关系数，然后将这些相关系数应用于每个单词的值来得到最终的注意力表示。

具体而言，自注意力机制通常通过以下步骤计算相关系数：

1. 输入：假设有一个输入序列，其中每个单词都表示为一个向量。

2. 线性映射：通过线性映射将每个单词的向量转换为查询（query）、键（key）和值（value）的向量表示。这通常通过矩阵乘法来实现，其中每个向量与相应的权重矩阵相乘。

3. 相似度计算：使用查询和键的向量表示计算每个单词与其他单词之间的相似度得分。常用的相似度计算方法是点积（dot product）或缩放的点积（scaled dot product），即将查询向量与键向量相乘，并进行适当的缩放和归一化处理。

4. 注意力权重计算：将相似度得分进行 softmax 归一化，以获得每个单词与其他单词的注意力权重。

5. 注意力表示计算：将注意力权重应用于值向量，通过加权求和的方式得到每个单词的最终注意力表示。

需要注意的是，自注意力机制并不需要单词与自身的乘积，而是通过计算相似度得分并将其应用于值向量来计算每个单词的注意力表示。这样可以在不引入过多参数的情况下，有效地捕捉输入序列中单词之间的相关性和语义关系。