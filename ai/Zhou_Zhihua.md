#	机器学习周志华; 
1.	https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E5%BF%97%E5%8D%8E.pdf
2.	啃书指导: https://www.bilibili.com/video/BV1wx411o7CK?p=2
	
#	前言;
#	序言; 
9.	
#	Chapter 1, 绪论;
##	1.1, 引言; 
(2019-7-24)
##	1.2, 基本术语; 
data set; instance; sample; feature vector; hypothesis; label; supervised learning, classification, regression; testing; unsupervised learning, clustering; generalization; distribution; (2019-7-25)

1.	机器学习、人工智能、深度学习的关系是什么？  
A from chatGPT: 机器学习（Machine Learning）是一种基于数据的算法，通过训练模型使其具备自我学习能力，从而自动从输入数据中提取规律和特征，并应用于分类、回归、聚类、预测等任务中。  
人工智能（Artificial Intelligence）是一个更为广泛的概念，它不仅包括机器学习，还包括其他形式的智能算法和方法，比如基于规则的系统、进化算法、知识表示与推理等等。人工智能的目标是通过模拟人类的智能行为和思维来实现智能化的计算机系统。   
深度学习（Deep Learning）是一种机器学习的分支，其主要是通过构建多层神经网络，实现对输入数据的分层抽象和特征学习，从而得到更加准确的预测和分类结果。深度学习主要应用于计算机视觉、语音识别、自然语言处理等领域，其许多模型和算法都是建立在机器学习和人工智能的基础上的。  
因此，**机器学习是人工智能的一个分支，而深度学习是机器学习的一个分支**。深度学习在很多任务中表现出色，也推动了人工智能的发展，但它仍然是机器学习的一部分，不能完全代表机器学习的全部。(2023年3月24日)

2. Q: 分类任务和回归任务的区别和联系是什么？  
A from chatGPT: 分类任务和回归任务是机器学习中两种不同的任务类型，它们的区别和联系如下：  
区别：  
**分类任务**是将输入数据划分到不同的预定义类别中，每个类别都有一个标签。通常分类任务的输出是一个离散的值，例如二分类任务（有两个类别）、多分类任务（有多个类别）等。而**回归任务**是根据输入数据预测一个连续的数值，例如房价预测等。  
在处理分类任务时，常用的评价指标是分类准确率，即分类正确的样本占总样本数的比例。而在回归任务中，通常使用均方误差（Mean Squared Error，MSE）或平均绝对误差（Mean Absolute Error，MAE）等指标来衡量模型的预测性能。  
联系：  
无论是分类任务还是回归任务，都需要将输入数据映射到输出结果。在机器学习中，通常使用训练数据集来训练模型，并使用测试数据集来评估模型的性能。  
一些模型可以**同时用于分类和回归**任务。例如，神经网络可以在输出层使用不同的激活函数来实现分类和回归任务。在深度学习中，一些模型，如卷积神经网络（Convolutional Neural Networks，CNNs）和循环神经网络（Recurrent Neural Networks，RNNs）也可以用于分类和回归任务。

3. Q: 有哪些可以同时用于分类任务和回归任务的模型？  
A from chatGPT: (1) 神经网络（Neural Network）：可以使用不同的输出层，例如softmax输出层用于分类任务，线性输出层用于回归任务。 (2) 卷积神经网络（Convolutional Neural Network）：可以使用不同的输出层和损失函数，例如softmax输出层和交叉熵损失函数用于分类任务，线性输出层和均方误差损失函数用于回归任务。(3) 循环神经网络（Recurrent Neural Network）：可以使用不同的输出层和损失函数，例如softmax输出层和交叉熵损失函数用于分类任务，线性输出层和均方误差损失函数用于回归任务。(4) 决策树（Decision Tree）：可以使用不同的损失函数，例如交叉熵用于分类任务，均方误差用于回归任务。(5) 支持向量机（Support Vector Machine）：可以使用不同的损失函数和核函数，例如线性核用于分类任务，高斯核用于回归任务。(6) 随机森林（Random Forest）：可以使用不同的损失函数，例如交叉熵用于分类任务，均方误差用于回归任务。

4. Q: 除了分类任务和回归任务，机器学习还有哪些典型任务？  
A from ChatGPT: (1) **聚类**(Clustering)：将数据集中的对象分成若干个组或者更小的簇，使得同一组内的对象相似度较高，而不同组之间的相似度较低。 (2) **降维**(Dimensionality Reduction)：将高维数据映射到低维空间，以便于可视化、数据处理和机器学习任务。(2023年3月24日)  



##	1.3, 假设空间; 
induction; deduction; version space; (2019-7-25)
##	1.4, 归纳偏好; 
no free lunch theorem; (2019-7-25)
##	1.5, 发展历程; 
学习过程涉及大量参数, 而参数的设置缺乏理论指导, 主要靠手工调参数; (2019-7-25)
##	1.6, 应用现状; 
data mining; (2019-7-25)
##	1.7, 阅读材料; 机器学习, 不显式编程地赋予计算机能力; (2019-7-25)
啃书指导P2, 绪论; 机器学习是什么, 利用经验改善系统性能; 模型决定上限, 数据决定下限; 机器学习做什么; 图片识别; 互联网推荐; 自动驾驶; 古文献修复; 帮助竞选; 监督学习VS无监督学习; 分类, 回归, 聚类; 四个特征组成一个特征向量; 版本空间; 没有最好的算法, 只有最适合的算法; (2020-11-12)
	
	
#	Chapter 2, 模型评估和选择; 
##	2.1, 经验误差和过拟合; 
overfitting; underfitting; model selection; (2019-7-25)
##	2.2, 评估方法; 
testing set; testing error; 留出法hold-out; 交叉验证法cross validation; 留一法leave-one-out; 自助法bootstrapping; 调参与最终模型; (2019-7-25)
##	2.3, 性能度量; 
均方误差mean square error; 错误率与精度; 查准率, 查全率和F1; ROC和AUC; 代价敏感错误率和代价曲线; (2019-7-25)
##	2.4, 比较检验; 
假设检验; 交叉验证t检验; MacNemar检验; Friedman检验和Nemenyi后续检验; (2019-7-25)
##	2.5, 偏差和方差; 
Comment: 原来偏差和方差的作用是来衡量泛化误差的, 用于模型评估. (2019-7-26)
##	2.6, 阅读材料; 
(2019-7-26) 
啃书指导P3, 模型评估; 训练集, 验证集(调整模型的超参数), 测试集; 混淆矩阵; bias, 期望值和实际值的差值; Variance, 方差; ROC曲线; (2020-11-14)
	
	
#	Chapter 3, 线性模型; 
##	3.1, 基本形式; 
(2019-7-26)
##	3.2, 线性回归; 
(2019-7-26)
##	3.3, 对数几率回归; 
chatGPT: 对数几率回归就是逻辑回归; 利用一个**线性模型**加上一个**逻辑函数**（Logistic Function，也叫sigmoid函数）来建立一个分类模型。逻辑回归的输出是一个概率值，表示样本属于某个类别的概率。在预测时，将这个概率与一个预先设定的阈值进行比较，大于阈值则将样本归为正类，小于阈值则将样本归为负类。 (2019-7-26)
1. Q: 什么是**极大似然估计**？极大似然估计一般用于机器学习的哪些场合？  
A from ChatGPT: **极大似然估计**（Maximum Likelihood Estimation，MLE）是一种统计方法，用于估计一个模型的**参数**，使得该模型的参数产生观测数据的概率最大。在数学上，MLE可以表示为一个优化问题，即最大化似然函数。  
在机器学习中，MLE通常用于监督学习中的**参数估计**，例如逻辑回归、线性回归等模型。在这些模型中，我们**希望估计一些参数，使得该模型的预测结果与真实标签的差别最小**。通常情况下，我们可以用MLE方法来估计这些参数。具体来说，我们可以先定义一个似然函数，然后最大化这个似然函数，得到最优参数。  
例如，对于二分类问题，我们可以使用逻辑回归模型，假设标签为0或1。对于给定的数据集，我们可以使用MLE方法来估计逻辑回归模型的参数。具体来说，我们可以定义似然函数为对所有样本的条件概率的乘积，然后使用梯度下降等方法来最大化这个似然函数。通过这种方法，我们可以获得最优的逻辑回归参数，用于进行分类任务。

2. Q: 除了极大似然估计，还有哪些常用的参数估计方法？  
A from ChatGPT: (1) **最小二乘法**：用于线性回归问题，目标是最小化观测数据与预测数据之间的平方误差；(2) **贝叶斯估计**：用于求解参数的后验概率分布，对比极大似然估计，贝叶斯估计能够更好地处理数据不充分的情况，并且能够量化参数的不确定性；(3) **最大后验估计**：在贝叶斯估计的基础上，加入先验概率分布，通过最大化后验概率分布来求解参数；(4) **最大间隔估计**：在支持向量机（SVM）中用于分类问题，通过最大化不同类别之间的边际距离来求解分类器参数。




##	3.4, 线性判别分析; 
类内散度矩阵; 类间散度矩阵; (2019-7-26)
##	3.5, 多分类学习; 
(2019-7-26)
##	3.6, 类别不平衡问题; 
(2019-7-26)
##	3.7, 阅读材料; 
(2019-7-26)
啃书指导P4, 线性模型; 线性回归; 最小二乘法; 逻辑回归, 表达一个事件发生的可能性; 最大似然估计, 每次猜对可能性的最大值; 目标函数, 最优化的损失函数; 模型最优化, 直接求解法(最小二乘法), 间接求解法(梯度下降法, 牛顿法); 正则化, 过拟合和欠拟合的trade-off; (2020-11-16)

1. Q: 深度学习中有哪些常用的信息论的概念？  
A from ChatGPT: **熵（Entropy）**：表示信息的不确定度，可以用来衡量一个分布的不确定程度。在深度学习中，经常用熵来度量一个分类器的输出的不确定性，或者用于衡量一个神经网络的输出的分布的不确定性。   
**交叉熵（Cross Entropy）**：在深度学习中，经常用于衡量两个概率分布之间的差异。例如，在分类问题中，交叉熵通常用于衡量分类器的预测结果和实际标签之间的差异。  
**相对熵（KL散度，Kullback-Leibler Divergence）**：用于衡量两个概率分布之间的差异，也称为KL散度。在深度学习中，经常使用KL散度来比较两个概率分布的相似性，例如，在生成模型中，KL散度可以用来衡量生成的样本分布和真实分布之间的差异。  
**互信息（Mutual Information）**：用于衡量两个随机变量之间的相互依赖关系。在深度学习中，互信息可以用来衡量两个神经网络的表示之间的相似性，或者用于度量生成模型中的隐变量和观测变量之间的相互依赖关系。  
**条件熵（Conditional Entropy）**：表示在已知一个变量的取值的情况下，另一个变量的不确定性。在深度学习中，条件熵经常用于衡量一个模型的预测在给定一些先验信息下的不确定性。  
**互信息增益（Information Gain）**：用于衡量在一个条件下，一个随机变量的不确定度减少的程度。在深度学习中，互信息增益可以用于选择最佳的特征来构建分类器。



#	Chapter 4, 决策树; 
##	4.1, 基本流程; 
(2019-7-26)
##	4.2, 划分选择; 
信息增益; 增益率; 基尼指数; (2019-7-26)
##	4.3, 剪枝处理; 
预剪枝; 后剪枝; (2019-7-26)
##	4.4, 连续与缺失值; 
连续值处理; 缺失值处理; (2019-7-26)
##	4.5, 多变量决策树; 
(2019-7-27)
##	4.6, 阅读材料; 
(2019-7-27)
啃书指导P5, 决策树; 分而治之, 从根到叶的递归; 可能性小的事情信息量大; 信息熵, 整个概率分布中的信息总量; 均匀分布信息熵高, 确定分布信息熵低; 信息熵越少纯度越高; 基尼系数是信息熵一阶泰勒展开式的一个近似; 和经济学中的基尼系数不是一回事; 信息熵越小, 纯度越高; 信息增益; 标签纯度越高, 信息熵越少; 增益率; (2020-11-19)

#	Chapter 5, 神经网络; 
##	5.1, 神经元模型; 
(2019-7-27)
##	5.2, 感知机与多层网络; 
多层前馈神经网络; (2019-7-27)
##	5.3, 误差逆传播算法; 
多层前馈网络能够以任意精度逼近任意复杂度的连续函数; (2019-7-27)
##	5.4, 全局最小和局部最小; 
(2019-7-27)
##	5.5, 其他常见神经网络; 
RBF网络; ART网络; SOM网络; 级联相关网络; Elman网络; Boltzmann机; (2019-7-27)
##	5.6, 深度学习; 
(2019-7-27)
##	5.7, 阅读材料; 
(2019-7-27)
啃书指导P6, 神经网络; 两层神经网络, 线性函数+max; 激活函数, 阶跃函数, sigmoid函数, ReLU函数(最常见); 没有激活函数, 神经网络就是线性函数; 误差逆传播; 链式求导; 正则化; (2020-12-1)
	
#	Chapter 6, 支持向量机;
##	6.1, 间隔与支持向量; 
(2019-7-27)
##	6.2, 对偶问题; 
(2019-7-27)
##	6.3, 核函数; 
定理6.1, 核函数; (2019-7-27)
##	6.4, 软间隔与正则化; 
(2019-7-27)
##	6.5, 支持向量回归; 
(2019-7-27)
##	6.6, 核方法; 
定理6.2, 表示定理; (2019-7-29)
##	6.7, 阅读材料; 
(2019-7-29)
	
#	Chapter 7, 贝叶斯分类器;
##	7.1, 贝叶斯决策论; 
(2019-7-29)
##	7.2, 极大似然估计; 
(2019-7-29)
##	7.3, 朴素贝叶斯分类器; 
(2019-7-29)
##	7.4, 半朴素贝叶斯分类器; 
(2019-7-29)
##	7.5, 贝叶斯网; 
结构; 学习; 推断; (2019-7-29)
##	7.6, EM算法; 
(2019-7-29)
##	7.7, 阅读材料; 
(2019-7-29)
	
#	Chapter 8, 集成学习;
##	8.1, 个体与集成; 
(2019-7-29)
##	8.2, boosting; 
AdaBoost算法; (2019-7-29)
##	8.3, bagging和随机森林; 
bagging; 随机森林; (2019-7-29)
##	8.4, 结合策略; 
平均法; 投票法; 学习法; (2019-7-29)
##	8.5, 多样性; 
多样性度量; 不合度量; 相关系数; Q统计量; 多样性增强; 数据样本扰动; 输入属性扰动; 输出表示扰动; 算法参数扰动; (2019-8-4)
##	8.6, 阅读材料; 
(2019-8-4)
	
#	Chapter 9, 聚类;
##	9.1, 聚类任务; 
(2019-8-4)
##	9.2, 性能度量; 
Jaccard系数; FM指数; Rand指数; DB指数; Dunn指数; (2019-8-4)
##	9.3, 距离计算; 
(2019-8-4)
##	9.4, 原型聚类; 
k均值算法; 学习向量量化; 高斯混合聚类; (2019-8-4)
##	9.5, 密度聚类; 
邻域; 核心对象; 密度直达; 密度可达; 密度相连; 连接性; 最大性; (2019-8-5)
##	9.6, 层次聚类; 
阅读材料; (2019-8-5)
	
#	Chapter 10, 降维与度量学习; 
##	10.1, k近邻学习; 
(2019-8-5)
##	10.2, 低维嵌入; 
多维缩放; (2019-8-5)
##	10.3, 主成分分析; 
(2019-8-5)
##	10.4, 核化线性降维; 
(2019-8-5)
##	10.5, 流形学习; 
等度量映射; 局部线性嵌入; (2019-8-5)
##	10.6, 度量学习; 
(2019-8-5)
##	10.7, 阅读材料; 
(2019-8-5)
	
	
#	Chapter 11, 特征选择与稀疏学习;
##	11.1, 子集搜索与评价; 
子集搜索; 子集评价; (2019-8-7)
##	11.2, 过滤式选择; 
(2019-8-7)
##	11.3, 包裹式选择; 
(2019-8-7)
##	11.4, 嵌入式选择与L1正则化; 
(2019-8-7)
##	11.5, 稀疏表示与字典学习; 
(2019-8-7)
##	11.6, 压缩感知; 
(2019-8-7)
##	11.7, 阅读材料; 
(2019-8-7)
	
#	Chapter 12, 计算学习理论;
##	12.1, 基础知识; 
Jensen不等式; Hoeffding不等式; McDiarmid不等式; (2019-8-7)
##	12.2, PAC学习; 
PAC identify; PAC learnable; PAC learning algorithm; sample complexity; (2019-8-7)
##	12.3, 有限假设空间; 
可分情形; 不可分情形; 不可知PAC可学习; (2019-8-7)
##	12.4, VC维; 
增长函数; (2019-8-7)
##	12.5, Rademacher复杂度; 
(2019-8-7)
##	12.6, 稳定性; 
(2019-8-7)
##	12.7, 阅读材料; 
(2019-8-7)
	
#	Chapter 13, 半监督学习;
##	13.1, 未标记样本; 
(2019-8-8)
##	13.2, 生成式方法; 
(2019-8-8)
##	13.3, 半监督SVM; 
(2019-8-8)
##	13.4, 图半监督学习; 
(2019-8-8)
##	13.5, 基于分歧的方法; 
(2019-8-8)
##	13.6, 半监督聚类; 
(2019-8-9)
##	13.7, 阅读材料; 
(2019-8-9)
	
#	Chapter 14, 概率图模型;
##	14.1, 隐马尔可夫模型; 
状态转移概率; 输出观测概率; 初始状态概率; (2019-8-9)
##	14.2, 马尔可夫随机场; 
局部马尔可夫性; 成对马尔可夫性; (2019-8-9)
##	14.3, 条件随机场; 
(2019-8-9)
##	14.4, 学习与推断; 
变量消去; 信念传播; (2019-8-9)
##	14.5, 近似推断; 
MCMC采样; 变分推断; (2019-8-20)
##	14.6, 话题模型; 
(2019-8-20)
##	14.7, 阅读材料; 
(2019-8-20)
	
#	Chapter 15, 规则学习;
##	15.1, 基本概念; 
(2019-8-20)
##	15.2, 序贯覆盖; 
(2019-8-20)
##	15.3, 剪枝优化; 
(2019-8-20)
##	15.4, 一阶规则学习; 
(2019-8-20)
##	15.5, 归纳逻辑程序设计; 
最小一般的泛化; 逆归结; (2019-8-20)
##	15.6, 阅读材料; 
(2019-8-20)
	
#	Chapter 16, 强化学习;
##	16.1, 任务与奖赏; 
(2019-8-20)
##	16.2, K-摇臂赌博机; 
探索与利用; 贪心; softmax; (2019-8-20)
##	16.3, 有模型学习; 
策略评估; 策略改进; 策略迭代与值迭代; (2019-8-20)
##	16.4, 免模型学习; 
蒙特卡洛强化学习; 时序差分学习; (2019-8-21)
##	16.5, 值函数近似; 
(2019-8-21)
##	16.6, 模仿学习; 
直接模仿学习; 逆强化学习; (2019-8-21)
##	16.7, 阅读材料; 
(2019-8-21)
	
#	附录;
#	A, 矩阵;
##	A.1, 基本演算; 
(2019-8-21)
##	A.2, 导数; 
(2019-8-21)
##	A.3, 奇异值分解; 
(2019-8-21)
#	B, 优化;
##	B.1, 拉格朗日乘子法; 
(2019-8-21)
##	B.2, 二次规划; 
(2019-8-21)
##	B.3, 半正定规划; 
(2019-8-21)
##	B.4, 梯度下降法; 
(2019-8-21)
##	B.5, 坐标下降法; 
(2019-8-21)
#	C, 概率分布;
##	C.1, 常见概率分布; 
均匀分布; 伯努利分布; 二项分布; 多项分布; 贝塔分布; 狄利克雷分布; 高斯分布; (2019-8-21)
##	C.2, 共轭分布; 
(2019-8-21)
##	C.3, KL散度; 
(2019-8-21)
	
#	后记;
	
#	索引;

