#	机器学习周志华; 
1.	https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E5%BF%97%E5%8D%8E.pdf
2.	啃书指导: https://www.bilibili.com/video/BV1wx411o7CK?p=2
	
#	前言;
#	序言; 
9.	
#	Chapter 1, 绪论;
##	1.1, 引言; 
(2019-7-24)
##	1.2, 基本术语; 
data set; instance; sample; feature vector; hypothesis; label; supervised learning, classification, regression; testing; unsupervised learning, clustering; generalization; distribution; (2019-7-25)
##	1.3, 假设空间; 
induction; deduction; version space; (2019-7-25)
##	1.4, 归纳偏好; 
no free lunch theorem; (2019-7-25)
##	1.5, 发展历程; 
学习过程涉及大量参数, 而参数的设置缺乏理论指导, 主要靠手工调参数; (2019-7-25)
##	1.6, 应用现状; 
data mining; (2019-7-25)
##	1.7, 阅读材料; 机器学习, 不显式编程地赋予计算机能力; (2019-7-25)
啃书指导P2, 绪论; 机器学习是什么, 利用经验改善系统性能; 模型决定上限, 数据决定下限; 机器学习做什么; 图片识别; 互联网推荐; 自动驾驶; 古文献修复; 帮助竞选; 监督学习VS无监督学习; 分类, 回归, 聚类; 四个特征组成一个特征向量; 版本空间; 没有最好的算法, 只有最适合的算法; (2020-11-12)
	
	
#	Chapter 2, 模型评估和选择; 
##	2.1, 经验误差和过拟合; 
overfitting; underfitting; model selection; (2019-7-25)
##	2.2, 评估方法; 
testing set; testing error; 留出法hold-out; 交叉验证法cross validation; 留一法leave-one-out; 自助法bootstrapping; 调参与最终模型; (2019-7-25)
##	2.3, 性能度量; 
均方误差mean square error; 错误率与精度; 查准率, 查全率和F1; ROC和AUC; 代价敏感错误率和代价曲线; (2019-7-25)
##	2.4, 比较检验; 
假设检验; 交叉验证t检验; MacNemar检验; Friedman检验和Nemenyi后续检验; (2019-7-25)
##	2.5, 偏差和方差; 
(2019-7-26)
##	2.6, 阅读材料; 
(2019-7-26) 
啃书指导P3, 模型评估; 训练集, 验证集(调整模型的超参数), 测试集; 混淆矩阵; bias, 期望值和实际值的差值; Variance, 方差; ROC曲线; (2020-11-14)
	
	
#	Chapter 3, 线性模型; 
##	3.1, 基本形式; 
(2019-7-26)
##	3.2, 线性回归; 
(2019-7-26)
##	3.3, 对数几率回归; 
(2019-7-26)
##	3.4, 线性判别分析; 
类内散度矩阵; 类间散度矩阵; (2019-7-26)
##	3.5, 多分类学习; 
(2019-7-26)
##	3.6, 类别不平衡问题; 
(2019-7-26)
##	3.7, 阅读材料; 
(2019-7-26)
啃书指导P4, 线性模型; 线性回归; 最小二乘法; 逻辑回归, 表达一个事件发生的可能性; 最大似然估计, 每次猜对可能性的最大值; 目标函数, 最优化的损失函数; 模型最优化, 直接求解法(最小二乘法), 间接求解法(梯度下降法, 牛顿法); 正则化, 过拟合和欠拟合的trade-off; (2020-11-16)
	
#	Chapter 4, 决策树; 
##	4.1, 基本流程; 
(2019-7-26)
##	4.2, 划分选择; 
信息增益; 增益率; 基尼指数; (2019-7-26)
##	4.3, 剪枝处理; 
预剪枝; 后剪枝; (2019-7-26)
##	4.4, 连续与缺失值; 
连续值处理; 缺失值处理; (2019-7-26)
##	4.5, 多变量决策树; 
(2019-7-27)
##	4.6, 阅读材料; 
(2019-7-27)
啃书指导P5, 决策树; 分而治之, 从根到叶的递归; 可能性小的事情信息量大; 信息熵, 整个概率分布中的信息总量; 均匀分布信息熵高, 确定分布信息熵低; 信息熵越少纯度越高; 基尼系数是信息熵一阶泰勒展开式的一个近似; 和经济学中的基尼系数不是一回事; 信息熵越小, 纯度越高; 信息增益; 标签纯度越高, 信息熵越少; 增益率; (2020-11-19)

#	Chapter 5, 神经网络; 
##	5.1, 神经元模型; 
(2019-7-27)
##	5.2, 感知机与多层网络; 
多层前馈神经网络; (2019-7-27)
##	5.3, 误差逆传播算法; 
多层前馈网络能够以任意精度逼近任意复杂度的连续函数; (2019-7-27)
##	5.4, 全局最小和局部最小; 
(2019-7-27)
##	5.5, 其他常见神经网络; 
RBF网络; ART网络; SOM网络; 级联相关网络; Elman网络; Boltzmann机; (2019-7-27)
##	5.6, 深度学习; 
(2019-7-27)
##	5.7, 阅读材料; 
(2019-7-27)
啃书指导P6, 神经网络; 两层神经网络, 线性函数+max; 激活函数, 阶跃函数, sigmoid函数, ReLU函数(最常见); 没有激活函数, 神经网络就是线性函数; 误差逆传播; 链式求导; 正则化; (2020-12-1)
	
#	Chapter 6, 支持向量机;
##	6.1, 间隔与支持向量; 
(2019-7-27)
##	6.2, 对偶问题; 
(2019-7-27)
##	6.3, 核函数; 
定理6.1, 核函数; (2019-7-27)
##	6.4, 软间隔与正则化; 
(2019-7-27)
##	6.5, 支持向量回归; 
(2019-7-27)
##	6.6, 核方法; 
定理6.2, 表示定理; (2019-7-29)
##	6.7, 阅读材料; 
(2019-7-29)
	
#	Chapter 7, 贝叶斯分类器;
##	7.1, 贝叶斯决策论; 
(2019-7-29)
##	7.2, 极大似然估计; 
(2019-7-29)
##	7.3, 朴素贝叶斯分类器; 
(2019-7-29)
##	7.4, 半朴素贝叶斯分类器; 
(2019-7-29)
##	7.5, 贝叶斯网; 
结构; 学习; 推断; (2019-7-29)
##	7.6, EM算法; 
(2019-7-29)
##	7.7, 阅读材料; 
(2019-7-29)
	
#	Chapter 8, 集成学习;
##	8.1, 个体与集成; 
(2019-7-29)
##	8.2, boosting; 
AdaBoost算法; (2019-7-29)
##	8.3, bagging和随机森林; 
bagging; 随机森林; (2019-7-29)
##	8.4, 结合策略; 
平均法; 投票法; 学习法; (2019-7-29)
##	8.5, 多样性; 
多样性度量; 不合度量; 相关系数; Q统计量; 多样性增强; 数据样本扰动; 输入属性扰动; 输出表示扰动; 算法参数扰动; (2019-8-4)
##	8.6, 阅读材料; 
(2019-8-4)
	
#	Chapter 9, 聚类;
##	9.1, 聚类任务; 
(2019-8-4)
##	9.2, 性能度量; 
Jaccard系数; FM指数; Rand指数; DB指数; Dunn指数; (2019-8-4)
##	9.3, 距离计算; 
(2019-8-4)
##	9.4, 原型聚类; 
k均值算法; 学习向量量化; 高斯混合聚类; (2019-8-4)
##	9.5, 密度聚类; 
邻域; 核心对象; 密度直达; 密度可达; 密度相连; 连接性; 最大性; (2019-8-5)
##	9.6, 层次聚类; 
阅读材料; (2019-8-5)
	
#	Chapter 10, 降维与度量学习; 
##	10.1, k近邻学习; 
(2019-8-5)
##	10.2, 低维嵌入; 
多维缩放; (2019-8-5)
##	10.3, 主成分分析; 
(2019-8-5)
##	10.4, 核化线性降维; 
(2019-8-5)
##	10.5, 流形学习; 
等度量映射; 局部线性嵌入; (2019-8-5)
##	10.6, 度量学习; 
(2019-8-5)
##	10.7, 阅读材料; 
(2019-8-5)
	
	
#	Chapter 11, 特征选择与稀疏学习;
##	11.1, 子集搜索与评价; 
子集搜索; 子集评价; (2019-8-7)
##	11.2, 过滤式选择; 
(2019-8-7)
##	11.3, 包裹式选择; 
(2019-8-7)
##	11.4, 嵌入式选择与L1正则化; 
(2019-8-7)
##	11.5, 稀疏表示与字典学习; 
(2019-8-7)
##	11.6, 压缩感知; 
(2019-8-7)
##	11.7, 阅读材料; 
(2019-8-7)
	
#	Chapter 12, 计算学习理论;
##	12.1, 基础知识; 
Jensen不等式; Hoeffding不等式; McDiarmid不等式; (2019-8-7)
##	12.2, PAC学习; 
PAC identify; PAC learnable; PAC learning algorithm; sample complexity; (2019-8-7)
##	12.3, 有限假设空间; 
可分情形; 不可分情形; 不可知PAC可学习; (2019-8-7)
##	12.4, VC维; 
增长函数; (2019-8-7)
##	12.5, Rademacher复杂度; 
(2019-8-7)
##	12.6, 稳定性; 
(2019-8-7)
##	12.7, 阅读材料; 
(2019-8-7)
	
#	Chapter 13, 半监督学习;
##	13.1, 未标记样本; 
(2019-8-8)
##	13.2, 生成式方法; 
(2019-8-8)
##	13.3, 半监督SVM; 
(2019-8-8)
##	13.4, 图半监督学习; 
(2019-8-8)
##	13.5, 基于分歧的方法; 
(2019-8-8)
##	13.6, 半监督聚类; 
(2019-8-9)
##	13.7, 阅读材料; 
(2019-8-9)
	
#	Chapter 14, 概率图模型;
##	14.1, 隐马尔可夫模型; 
状态转移概率; 输出观测概率; 初始状态概率; (2019-8-9)
##	14.2, 马尔可夫随机场; 
局部马尔可夫性; 成对马尔可夫性; (2019-8-9)
##	14.3, 条件随机场; 
(2019-8-9)
##	14.4, 学习与推断; 
变量消去; 信念传播; (2019-8-9)
##	14.5, 近似推断; 
MCMC采样; 变分推断; (2019-8-20)
##	14.6, 话题模型; 
(2019-8-20)
##	14.7, 阅读材料; 
(2019-8-20)
	
#	Chapter 15, 规则学习;
141.	15.1, 基本概念; (2019-8-20)
142.	15.2, 序贯覆盖; (2019-8-20)
143.	15.3, 剪枝优化; (2019-8-20)
144.	15.4, 一阶规则学习; (2019-8-20)
145.	15.5, 归纳逻辑程序设计; 最小一般的泛化; 逆归结; (2019-8-20)
146.	15.6, 阅读材料; (2019-8-20)
147.	
148.	Chapter 16, 强化学习;
149.	16.1, 任务与奖赏; (2019-8-20)
150.	16.2, K-摇臂赌博机; 探索与利用; 贪心; softmax; (2019-8-20)
151.	16.3, 有模型学习; 策略评估; 策略改进; 策略迭代与值迭代; (2019-8-20)
152.	16.4, 免模型学习; 蒙特卡洛强化学习; 时序差分学习; (2019-8-21)
153.	16.5, 值函数近似; (2019-8-21)
154.	16.6, 模仿学习; 直接模仿学习; 逆强化学习; (2019-8-21)
155.	16.7, 阅读材料; (2019-8-21)
156.	
157.	附录;
158.	A, 矩阵;
159.	A.1, 基本演算; (2019-8-21)
160.	A.2, 导数; (2019-8-21)
161.	A.3, 奇异值分解; (2019-8-21)
162.	B, 优化;
163.	B.1, 拉格朗日乘子法; (2019-8-21)
164.	B.2, 二次规划; (2019-8-21)
165.	B.3, 半正定规划; (2019-8-21)
166.	B.4, 梯度下降法; (2019-8-21)
167.	B.5, 坐标下降法; (2019-8-21)
168.	C, 概率分布;
169.	C.1, 常见概率分布; 均匀分布; 伯努利分布; 二项分布; 多项分布; 贝塔分布; 狄利克雷分布; 高斯分布; (2019-8-21)
170.	C.2, 共轭分布; (2019-8-21)
171.	C.3, KL散度; (2019-8-21)
172.	
173.	后记;
174.	
175.	索引;
176.	-
